{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11: Testing with Out-of-Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/yutanagano/Projects/nnfs')\n",
    "os.chdir('/home/yutanagano/Projects/nnfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "import nnn\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create the necessary layers\n",
    "dense1 = nnn.layer.Dense(n_inputs=2,n_neurons=64)\n",
    "relu = nnn.activation.Relu()\n",
    "dense2 = nnn.layer.Dense(n_inputs=64,n_neurons=3)\n",
    "activation_loss = nnn.loss.SoftmaxWithCategoricalCrossentropy()\n",
    "\n",
    "# Create the optimiser\n",
    "optimiser = nnn.optimiser.Adam(learning_rate=0.01, decay=0.00001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.01\n",
      "epoch: 100, acc: 0.703, loss: 0.707, lr: 0.009990109791306606\n",
      "epoch: 200, acc: 0.763, loss: 0.581, lr: 0.009980139522350523\n",
      "epoch: 300, acc: 0.830, loss: 0.439, lr: 0.009970189134487882\n",
      "epoch: 400, acc: 0.853, loss: 0.414, lr: 0.009960258568312435\n",
      "epoch: 500, acc: 0.860, loss: 0.375, lr: 0.009950347764654375\n",
      "epoch: 600, acc: 0.920, loss: 0.249, lr: 0.009940456664579172\n",
      "epoch: 700, acc: 0.930, loss: 0.217, lr: 0.009930585209386389\n",
      "epoch: 800, acc: 0.887, loss: 0.269, lr: 0.009920733340608539\n",
      "epoch: 900, acc: 0.933, loss: 0.198, lr: 0.00991090100000991\n",
      "epoch: 1000, acc: 0.943, loss: 0.177, lr: 0.009901088129585442\n",
      "epoch: 1100, acc: 0.927, loss: 0.185, lr: 0.00989129467155956\n",
      "epoch: 1200, acc: 0.717, loss: 1.383, lr: 0.009881520568385064\n",
      "epoch: 1300, acc: 0.953, loss: 0.163, lr: 0.009871765762741982\n",
      "epoch: 1400, acc: 0.950, loss: 0.153, lr: 0.009862030197536465\n",
      "epoch: 1500, acc: 0.957, loss: 0.146, lr: 0.009852313815899663\n",
      "epoch: 1600, acc: 0.957, loss: 0.140, lr: 0.009842616561186627\n",
      "epoch: 1700, acc: 0.963, loss: 0.135, lr: 0.009832938376975192\n",
      "epoch: 1800, acc: 0.963, loss: 0.130, lr: 0.009823279207064904\n",
      "epoch: 1900, acc: 0.943, loss: 0.177, lr: 0.009813638995475911\n",
      "epoch: 2000, acc: 0.953, loss: 0.141, lr: 0.009804017686447907\n",
      "epoch: 2100, acc: 0.963, loss: 0.126, lr: 0.009794415224439025\n",
      "epoch: 2200, acc: 0.963, loss: 0.121, lr: 0.009784831554124797\n",
      "epoch: 2300, acc: 0.963, loss: 0.117, lr: 0.00977526662039707\n",
      "epoch: 2400, acc: 0.967, loss: 0.113, lr: 0.009765720368362972\n",
      "epoch: 2500, acc: 0.747, loss: 1.130, lr: 0.009756192743343836\n",
      "epoch: 2600, acc: 0.957, loss: 0.120, lr: 0.009746683690874182\n",
      "epoch: 2700, acc: 0.967, loss: 0.110, lr: 0.009737193156700649\n",
      "epoch: 2800, acc: 0.967, loss: 0.107, lr: 0.009727721086781\n",
      "epoch: 2900, acc: 0.967, loss: 0.104, lr: 0.009718267427283064\n",
      "epoch: 3000, acc: 0.967, loss: 0.101, lr: 0.009708832124583735\n",
      "epoch: 3100, acc: 0.967, loss: 0.099, lr: 0.009699415125267946\n",
      "epoch: 3200, acc: 0.853, loss: 0.691, lr: 0.009690016376127677\n",
      "epoch: 3300, acc: 0.957, loss: 0.124, lr: 0.00968063582416093\n",
      "epoch: 3400, acc: 0.957, loss: 0.116, lr: 0.00967127341657076\n",
      "epoch: 3500, acc: 0.957, loss: 0.112, lr: 0.009661929100764258\n",
      "epoch: 3600, acc: 0.957, loss: 0.107, lr: 0.009652602824351587\n",
      "epoch: 3700, acc: 0.960, loss: 0.103, lr: 0.009643294535144986\n",
      "epoch: 3800, acc: 0.960, loss: 0.099, lr: 0.009634004181157814\n",
      "epoch: 3900, acc: 0.910, loss: 0.222, lr: 0.009624731710603567\n",
      "epoch: 4000, acc: 0.963, loss: 0.104, lr: 0.009615477071894923\n",
      "epoch: 4100, acc: 0.963, loss: 0.100, lr: 0.009606240213642783\n",
      "epoch: 4200, acc: 0.963, loss: 0.097, lr: 0.009597021084655323\n",
      "epoch: 4300, acc: 0.963, loss: 0.095, lr: 0.009587819633937046\n",
      "epoch: 4400, acc: 0.967, loss: 0.092, lr: 0.009578635810687842\n",
      "epoch: 4500, acc: 0.967, loss: 0.090, lr: 0.00956946956430205\n",
      "epoch: 4600, acc: 0.967, loss: 0.089, lr: 0.009560320844367537\n",
      "epoch: 4700, acc: 0.960, loss: 0.104, lr: 0.009551189600664763\n",
      "epoch: 4800, acc: 0.963, loss: 0.094, lr: 0.00954207578316587\n",
      "epoch: 4900, acc: 0.967, loss: 0.089, lr: 0.009532979342033765\n",
      "epoch: 5000, acc: 0.967, loss: 0.087, lr: 0.009523900227621215\n",
      "epoch: 5100, acc: 0.967, loss: 0.086, lr: 0.009514838390469937\n",
      "epoch: 5200, acc: 0.967, loss: 0.085, lr: 0.009505793781309708\n",
      "epoch: 5300, acc: 0.967, loss: 0.083, lr: 0.009496766351057465\n",
      "epoch: 5400, acc: 0.967, loss: 0.082, lr: 0.009487756050816422\n",
      "epoch: 5500, acc: 0.930, loss: 0.208, lr: 0.009478762831875183\n",
      "epoch: 5600, acc: 0.967, loss: 0.096, lr: 0.009469786645706873\n",
      "epoch: 5700, acc: 0.967, loss: 0.089, lr: 0.009460827443968249\n",
      "epoch: 5800, acc: 0.967, loss: 0.086, lr: 0.009451885178498853\n",
      "epoch: 5900, acc: 0.967, loss: 0.084, lr: 0.009442959801320124\n",
      "epoch: 6000, acc: 0.967, loss: 0.082, lr: 0.009434051264634572\n",
      "epoch: 6100, acc: 0.967, loss: 0.081, lr: 0.009425159520824889\n",
      "epoch: 6200, acc: 0.967, loss: 0.080, lr: 0.009416284522453132\n",
      "epoch: 6300, acc: 0.967, loss: 0.078, lr: 0.009407426222259851\n",
      "epoch: 6400, acc: 0.967, loss: 0.077, lr: 0.009398584573163282\n",
      "epoch: 6500, acc: 0.967, loss: 0.075, lr: 0.00938975952825848\n",
      "epoch: 6600, acc: 0.877, loss: 0.364, lr: 0.009380951040816517\n",
      "epoch: 6700, acc: 0.967, loss: 0.080, lr: 0.009372159064283639\n",
      "epoch: 6800, acc: 0.967, loss: 0.077, lr: 0.009363383552280452\n",
      "epoch: 6900, acc: 0.967, loss: 0.075, lr: 0.009354624458601109\n",
      "epoch: 7000, acc: 0.967, loss: 0.074, lr: 0.009345881737212498\n",
      "epoch: 7100, acc: 0.967, loss: 0.073, lr: 0.009337155342253428\n",
      "epoch: 7200, acc: 0.967, loss: 0.072, lr: 0.009328445228033845\n",
      "epoch: 7300, acc: 0.967, loss: 0.071, lr: 0.00931975134903401\n",
      "epoch: 7400, acc: 0.970, loss: 0.070, lr: 0.009311073659903723\n",
      "epoch: 7500, acc: 0.970, loss: 0.068, lr: 0.009302412115461537\n",
      "epoch: 7600, acc: 0.970, loss: 0.067, lr: 0.009293766670693966\n",
      "epoch: 7700, acc: 0.977, loss: 0.065, lr: 0.009285137280754698\n",
      "epoch: 7800, acc: 0.977, loss: 0.063, lr: 0.009276523900963831\n",
      "epoch: 7900, acc: 0.973, loss: 0.062, lr: 0.009267926486807106\n",
      "epoch: 8000, acc: 0.977, loss: 0.061, lr: 0.00925934499393513\n",
      "epoch: 8100, acc: 0.977, loss: 0.060, lr: 0.009250779378162611\n",
      "epoch: 8200, acc: 0.977, loss: 0.058, lr: 0.00924222959546761\n",
      "epoch: 8300, acc: 0.977, loss: 0.057, lr: 0.009233695601990784\n",
      "epoch: 8400, acc: 0.977, loss: 0.056, lr: 0.009225177354034633\n",
      "epoch: 8500, acc: 0.943, loss: 0.167, lr: 0.009216674808062748\n",
      "epoch: 8600, acc: 0.950, loss: 0.106, lr: 0.009208187920699086\n",
      "epoch: 8700, acc: 0.957, loss: 0.093, lr: 0.00919971664872722\n",
      "epoch: 8800, acc: 0.963, loss: 0.086, lr: 0.009191260949089605\n",
      "epoch: 8900, acc: 0.967, loss: 0.082, lr: 0.009182820778886859\n",
      "epoch: 9000, acc: 0.967, loss: 0.079, lr: 0.009174396095377022\n",
      "epoch: 9100, acc: 0.967, loss: 0.076, lr: 0.00916598685597485\n",
      "epoch: 9200, acc: 0.967, loss: 0.073, lr: 0.009157593018251083\n",
      "epoch: 9300, acc: 0.967, loss: 0.071, lr: 0.009149214539931748\n",
      "epoch: 9400, acc: 0.967, loss: 0.070, lr: 0.009140851378897431\n",
      "epoch: 9500, acc: 0.967, loss: 0.068, lr: 0.009132503493182587\n",
      "epoch: 9600, acc: 0.970, loss: 0.066, lr: 0.009124170840974827\n",
      "epoch: 9700, acc: 0.970, loss: 0.064, lr: 0.009115853380614228\n",
      "epoch: 9800, acc: 0.970, loss: 0.063, lr: 0.009107551070592627\n",
      "epoch: 9900, acc: 0.937, loss: 0.177, lr: 0.009099263869552954\n",
      "epoch: 10000, acc: 0.963, loss: 0.078, lr: 0.009090991736288512\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    output = dense1.forward(X)\n",
    "    output = relu.forward(output)\n",
    "    output = dense2.forward(output)\n",
    "\n",
    "    # Calculate the network's current loss\n",
    "    loss = activation_loss.forward(output, y)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    predictions = np.argmax(output,axis=1)\n",
    "    if len(y.shape) == 2: y = np.argmax(y,axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    # Print accuracy\n",
    "    if not epoch % 100: print(f\"epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, lr: {optimiser.current_learning_rate}\")\n",
    "\n",
    "    # Backward pass\n",
    "    activation_loss.backward(activation_loss.outputs, y)\n",
    "    dense2.backward(activation_loss.dinputs)\n",
    "    relu.backward(dense2.dinputs)\n",
    "    dense1.backward(relu.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimiser.pre_update_params()\n",
    "    optimiser.update_params(dense1)\n",
    "    optimiser.update_params(dense2)\n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the validation data set\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass on validation set\n",
    "output = dense1.forward(X_test)\n",
    "output = relu.forward(output)\n",
    "output = dense2.forward(output)\n",
    "\n",
    "# Calculate validation loss\n",
    "loss = activation_loss.forward(output, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.803, loss: 0.975\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "predictions = np.argmax(output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f\"validation, acc: {accuracy:.3f}, loss: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d485a98a36d7387b9a90aa3c793ce7a86a2239fa29b78a2ccbe0584c0791300d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
